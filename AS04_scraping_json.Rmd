---
title: "AS04_scraping_json"
author: "YOUR_NAME"
date: "10/10/2019"
output:
  html_document:
    highlight: zenburn
    number_sections: no
    theme: cerulean
    toc: yes
    css: style.css
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Loading packages
```{r}
library(tidyverse)
library(httr)
library(jsonlite)
library(lubridate)
options(stringsAsFactors = F)
```


# Scraping ubike data
- Here is the ubike json data link https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json
- You are asked to scrape youbike data per 5 minutes, continuing at least 12 hours.
- Saving each ubike json per 5 minutes with file name containing timestamp, which means you should have at least 12*12 files

```{r hints}
url <- "https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json"
GET(url) %>% content("text") %>% cat(file = "test.json")
read_json("test.json") %>% class

for(i in 1:5){
    message(i, "\t", now())
    Sys.sleep(3)
}

# Converting datetime to character
now() %>% format("%Y%m%d%H%M%S")

# Listing all files in a sub-folder
list.files("data/", ".*\\.rds") 

# If you pud your data in a sub-folder, you need to use full name to access them.
list.files("data/", ".*\\.rds", full.names = T) 
```


## **Q1.1 ANS** list files
- Using `list.files()` to list json files you scraped
- Using `length` to calculate `list.files()` result to see how many json files you have.

```{r}
# your code
```

## Q1.2
- Reading JSON files one by one
- Converting them to data frame individually
- Defining one indicator: fullness = sbi/tot
- Calculating each site's fullness by time

```{r}
# your code 
```


## **Q1.2 ANS**
- Using `geom_line` to display all site's fullness by time

```{r}
# your code
```



# Q2.Scraping dcard forum (No extra point if you have solved the Q1 succesfully)
- (If you cannot solve Q1 successfully, try to solve the Q2)
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- e.g., https://www.dcard.tw/f/relationship
- One video tutorial has introduced how to find out dcard forum's JSON data page by page.
- Adding code chunks as you need below

## **Q2.1.ANS** Print out class and dimension of your data
```{r}
# your code here
```


## **Q2.2.ANS** Using bar chart to show the number of post trend by week.
```{r}
# your code here
```



# (No extra point) Discovering one more website generated by JSON
- Try to find another website or webpage which is generated by JSON.
- Website title: [Fill-in-here]
- Website url: [Fill-in-here]
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- Adding code chunks as you need below

## **Q3.ANS** Print out glimpse(), class, and dimension of your data
```{r}
# YOUR CODE SHOULD BE HERE
```

