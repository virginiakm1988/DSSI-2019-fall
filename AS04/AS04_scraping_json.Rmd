---
title: "AS04_scraping_json"
author: "Virginia Chen"
date: "10/10/2019"
output:
  html_document:
    highlight: zenburn
    number_sections: no
    theme: cerulean
    toc: yes
    css: style.css
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rmarkdown::pandoc_version()
```


# Loading packages
```{r}
library(tidyverse)
library(httr)
library(jsonlite)
library(lubridate)
options(stringsAsFactors = F)
options(encoding = "utf8")
```


# Scraping ubike data
- Here is the ubike json data link https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json
- You are asked to scrape youbike data per 5 minutes, continuing at least 12 hours.
- Saving each ubike json per 5 minutes with file name containing timestamp, which means you should have at least 12*12 files

```{r hints}
url <- "https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json"
GET(url) %>% content("text") %>% cat(file = "data/test.json")
read_json("data/test.json") %>% class


#for(i in 1:144){
#    message(i, "\t", now())
#    time<-now() %>% format("%Y%m%d%H%M%S")
#    filename <- paste("data/",time , ".json",sep = "")
#    GET(url) %>% content("text") %>% cat(file = filename)
#    Sys.sleep(300)
#}
# Converting datetime to character
now() %>% format("%Y%m%d%H%M%S")

# Listing all files in a sub-folder
list.files("data/", ".*\\.rds") 

# If you pud your data in a sub-folder, you need to use full name to access them.
list.files("data/", ".*\\.rds", full.names = T) 
```


## **Q1.1 ANS** list files
- Using `list.files()` to list json files you scraped
- Using `length` to calculate `list.files()` result to see how many json files you have.

```{r}
# your code
# Listing all files in a sub-folder
list.files("data/", ".*\\.json") 
length(list.files("data/", ".*\\.json"))
```

## Q1.2
- Reading JSON files one by one
- Converting them to data frame individually
- Defining one indicator: fullness = sbi/tot
- Calculating each site's fullness by time

```{r}
# your code 
bike.df <- data.frame()
fname<-list.files("data/", ".*\\.json",full.names = T)

df_all <- data.frame()
for (file in fname){
  df <- read_lines(file, locale =locale(encoding="UTF-8"))%>% 
      fromJSON()
    df <- df$retVal %>% as.data.frame()
    df <- df %>%
        gather(key=key,value = value) %>%
        separate(col = "key", into=c("code", "var")) %>%
        spread(key="var", value="value")
    df <- df %>%
        mutate("sbi" = as.numeric(sbi), "tot"=as.numeric(tot)) %>%
        mutate("fullness"= sbi/tot) %>%
        mutate("mday"= ymd_hms(mday))
    
    df_all <- bind_rows(df_all, df)
}


```


## **Q1.2 ANS**
- Using `geom_line` to display all site's fullness by time

```{r}
# your code
test_df <- df_all%>%
  filter(mday > ymd("20191020"), sarea=="中正區")
ggplot(test_df,aes(mday, fullness, col=sna)) +
  geom_line(show.legend = F)

```



# Q2.Scraping dcard forum (No extra point if you have solved the Q1 succesfully)
- (If you cannot solve Q1 successfully, try to solve the Q2)
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- e.g., https://www.dcard.tw/f/relationship
- One video tutorial has introduced how to find out dcard forum's JSON data page by page.
- Adding code chunks as you need below

## **Q2.1.ANS** Print out class and dimension of your data
```{r}
# your code here
```


## **Q2.2.ANS** Using bar chart to show the number of post trend by week.
```{r}
# your code here
```



# (No extra point) Discovering one more website generated by JSON
- Try to find another website or webpage which is generated by JSON.
- Website title: [Fill-in-here]
- Website url: [Fill-in-here]
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- Adding code chunks as you need below

## **Q3.ANS** Print out glimpse(), class, and dimension of your data
```{r}
# YOUR CODE SHOULD BE HERE
```

